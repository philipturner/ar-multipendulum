# AR MultiPendulum

AR MultiPendulum allows users to interact with virtual objects directly with their hand instead of tapping an object's location on a touchscreen. It brings a mesmerizing multi-pendulum simulation into augmented reality. Users interact with this simulation through hand movements and modify it through a holographic interface.

Not only does this app bring augmented reality to a pendulum simulation, it is also the first app to simulate more than two pendulums. Additionally, by repurposing a VR headset for AR, this is the first app that gives users an affordable AR headset experience. If a user has Google Cardboard, they can turn their iPhone into an AR headset that renders their surroundings in VR, based on images acquired using their deviceâ€™s camera.

Some of the latest iOS devices have a built-in LiDAR scanner, which allows a device to understand the 3D shape of its surroundings. On these devices, this app uses the scanner to more realistically render the user's surroundings and reconstruct the 3D position of their hand.

This repository contains source code created during [this](https://github.com/philipturner/scene-color-reconstruction) research on scene color reconstruction. This source code uses the term "mixed reality" (MR) to differentiate code that generates the handheld experience from code that generates the headset experience. This term is a synonym for augmented reality often used to describe AR headset experiences.

A browser version of the multi-pendulum simulation can be accessed [here](https://github.com/philipturner/multipendulum).
